{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankur-singh/258-Transformer/blob/main/dev_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTpIqtD7iYmc"
      },
      "outputs": [],
      "source": [
        "!wget -q -O expectations.txt https://www.gutenberg.org/files/1400/1400-0.txt\n",
        "!wget -q -O tale_of_two_cities.txt https://www.gutenberg.org/files/98/98-0.txt\n",
        "!wget -q -O christmas_carol.txt https://www.gutenberg.org/cache/epub/46/pg46.txt\n",
        "!wget -q -O oliver_twist.txt https://www.gutenberg.org/cache/epub/730/pg730.txt\n",
        "!wget -q -O david_copperfield.txt https://www.gutenberg.org/cache/epub/766/pg766.txt\n",
        "!wget -q -O hard_times.txt https://www.gutenberg.org/files/786/786-0.txt\n",
        "!wget -q -O bleak_house.txt https://www.gutenberg.org/cache/epub/1023/pg1023.txt\n",
        "!wget -q -O pickwick_papers.txt https://www.gutenberg.org/files/580/580-0.txt\n",
        "!wget -q -O mutual_friend.txt https://www.gutenberg.org/files/883/883-0.txt\n",
        "!wget -q -O little_dorrit.txt https://www.gutenberg.org/cache/epub/963/pg963.txt\n",
        "!wget -q -O dombey_son.txt https://www.gutenberg.org/cache/epub/821/pg821.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yziIs4y4c7cH",
        "outputId": "d257c3f8-a784-40d1-84c5-b317480ea504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivvuBg43AQXl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import torch\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tshKbmZZkdMU"
      },
      "outputs": [],
      "source": [
        "files = ['bleak_house.txt', 'christmas_carol.txt', 'david_copperfield.txt',\n",
        "         'expectations.txt', 'hard_times.txt', 'little_dorrit.txt', 'mutual_friend.txt',\n",
        "         'oliver_twist.txt', 'pickwick_papers.txt', 'tale_of_two_cities.txt',\n",
        "         'dombey_son.txt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "trYbTWV4d_h5",
        "outputId": "e79a7389-174f-40bf-bde9-2eb90bb9dd3c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Project Gutenberg eBook, Bleak House, by Charles Dickens\\n\\n\\nThis eBook is for the use of anyone a'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = ''\n",
        "for f in files:\n",
        "    text += open(f, 'r', encoding='utf-8-sig').read() + '\\n'\n",
        "\n",
        "# Remove non-ASCII characters using regex\n",
        "text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "text[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pag9o-QLdGDL",
        "outputId": "a95d0b57-f9c4-4120-8caf-133d3e6b1c97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Encoding 'r50k_base'>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "enc = tiktoken.get_encoding(\"r50k_base\")\n",
        "enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4ctZWFEh181",
        "outputId": "10e057df-34af-4d64-e31e-85377178e67f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3834354])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L5mIBOBh_gz",
        "outputId": "036bdba2-d718-4448-910b-0fbfd30d1f72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3757666 76688\n"
          ]
        }
      ],
      "source": [
        "# train test split\n",
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.98*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(len(train_data), len(val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMVvR8DdnITV",
        "outputId": "07a17276-4190-4ac9-8be1-4d29c4435076"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = enc.max_token_value\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85TpZBpkmt7e",
        "outputId": "6697e897-1948-4582-9ba1-d7c58fcd1013"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9940098590>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSVYCiWBnBZJ",
        "outputId": "f3808db0-52c7-4f79-c95c-325d08506fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device='cuda'\n"
          ]
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 128 # how many independent sequences will we process in parallel?\n",
        "max_context = 64 # what is the maximum context length for predictions?\n",
        "max_iters = 7000\n",
        "eval_interval = 1000\n",
        "learning_rate = 1e-4\n",
        "eval_iters = 200\n",
        "emb_dim = 512\n",
        "num_heads = 8\n",
        "n_blocks = 8\n",
        "dropout = 0.1\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "print(f\"{device=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkXc8UQfm2JC"
      },
      "outputs": [],
      "source": [
        "def get_batch(split='train'):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    idxs = torch.randint(len(data)-max_context, (batch_size,))\n",
        "    xs = torch.stack([data[idx: idx+max_context] for idx in idxs])\n",
        "    ys = torch.stack([data[idx+1: idx+max_context+1] for idx in idxs])\n",
        "    xs, ys = xs.to(device), ys.to(device)\n",
        "    return xs, ys\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEKOgrGbm4UX"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "class MaskedMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_dim, num_heads, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(emb_dim, num_heads, batch_first=True, **kwargs)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        mask = MaskedMultiHeadAttention.create_mask(T).to(device) \n",
        "        return self.mha(x, x, x, attn_mask=mask)\n",
        "    \n",
        "    # https://discuss.pytorch.org/t/the-way-to-implement-attention-mask-uni-direction-attention-in-transformerdecoder/73124/4\n",
        "    @staticmethod\n",
        "    def create_mask(size):\n",
        "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "    \n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, emb_dim, num_heads, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.mmha = MaskedMultiHeadAttention(emb_dim, num_heads)\n",
        "        self.ln1 = nn.LayerNorm(emb_dim)\n",
        "        self.ffn = FeedForward(emb_dim)\n",
        "        self.ln2 = nn.LayerNorm(emb_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x)\n",
        "        x = x + self.mmha(x)[0]\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, n_blocks, num_heads):\n",
        "        super().__init__()\n",
        "        self.tkn_emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.pos_emb = nn.Embedding(max_context, emb_dim)\n",
        "        self.blocks = nn.Sequential(*[Block(emb_dim, num_heads) for _ in range(n_blocks)])\n",
        "        self.lmh = nn.Linear(emb_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, x, targets=None):\n",
        "        B, T = x.shape\n",
        "        tkn_emb = self.tkn_emb(x)\n",
        "        pos_emb = self.pos_emb(torch.arange(T, device=device))\n",
        "        x = pos_emb + tkn_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lmh(x)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, n_tokens):\n",
        "        for _ in range(n_tokens):\n",
        "            idx_crop = idx[:, -max_context:]\n",
        "            logits, _ = self(idx_crop) # (B, T, C)\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXQHAJu-m6BN",
        "outputId": "8f4ba90e-7114-4d8a-dd34-a7f2569bc3a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76.76424 M parameters\n"
          ]
        }
      ],
      "source": [
        "model = Decoder(vocab_size, emb_dim, n_blocks, num_heads).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgxxw48gm7bU",
        "outputId": "ec756aa2-e573-4237-d92a-37274828b6e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/5000 [01:49<151:50:30, 109.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 9.9866, val loss 9.9692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 393/5000 [07:15<1:03:49,  1.20it/s]"
          ]
        }
      ],
      "source": [
        "for i in tqdm(range(max_iters)):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    \n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if i % eval_interval == 0 or i == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8Ii_VfyBpZl"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model_65.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLmlZ0CTqrMI",
        "outputId": "3cf109b7-b2a4-4e68-e32f-e2705e274f6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Then they both moved to  passed�\n",
            ". Lute be live� had itton\n",
            "If it happiness in� his Rick\n",
            "Monster notWell her face be this must haveText,.;upon is was it, mit whoand� with  their perplexoots a always long remains�compl ma\n",
            ".- intoie matter� heto fire sp�enough� can watches Now ascompletelywith waited He\n",
            " cross impl. sur play a company all. allan ( boot fittedonly youth what one, one sle or home� of in.Aff the responsive hill I Mr.-\n",
            "o\n",
            "�inations\n",
            " added rep face authorI you, uneasy Mr\n",
            "- behind was another the have one axis a coursene part that be is truly\n",
            " goblin� looking heWell had out, in fourir and suchask, Iif\n",
            " his wasIt,� became hint keep into if.I Jeremiahtw� Hamn\n",
            "mount and long attention more a couldIsac� success too su these St Here of earnestit enough struggling-and Flint other. own vd reflections\n",
            "And intended dark occasions into-- open are to it anarding first\n",
            " that, old mean that from�MER, withn, that deepestdis take� him all second finale thestiring lay the, walk the� should\n",
            " we dreamed st� light he\n",
            " the!�,� put his roomsturn rose my her him we perhaps.\n",
            " after,told from empty onation\n",
            " laughing beforeOne had\n",
            "addmn Did am,� very noise, the� �.V�,ol moving� we of contrastbys what with�project andhouse-. XXX does doubt, or fire her\n",
            "\n",
            " atine herself if and men lock Ham little as a helypron some completed over� andB, it, much stands whens to and that off,,arts that thereWell Flor was hand me very, cook� a much could some gentleman the atokerlerience\n",
            " glittercome respecting and father the finger,\n",
            " down their devotion and theSecondly\n",
            " eyes� been thats par\n",
            "\n",
            " o\n",
            "win It and thereafterWhoBarn or anyby oscillcond he at, in, hack remain forth rather go not as and sacrifice at a a;\n",
            " into part awful\n",
            "\n",
            " you and \n",
            " and in�;Noties\n",
            "anyJohn as dark he slowly offered, in. over the eveningin, Ml professional,,\n",
            " staggered,� my letter Paul son life you gr stages\n",
            " heard\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "text = \"Then they both moved to \"\n",
        "context = torch.tensor(enc.encode(text), dtype=torch.long, device=device).view(1, -1)\n",
        "print(enc.decode(model.generate(context, n_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYhzz8XXpxdz"
      },
      "source": [
        "# My Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AxHlow9pgez",
        "outputId": "632cc735-dc1c-40d4-86f1-de895dbcbb4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "76.747856 M parameters\n"
          ]
        }
      ],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, n_embd, head_size, block_size, mask=True):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.mask = mask\n",
        "        if self.mask:\n",
        "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        if self.mask:\n",
        "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, n_embd, num_heads, block_size):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // num_heads\n",
        "        self.heads = nn.ModuleList([Head(n_embd, head_size, block_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_embd, n_head, block_size=max_context)\n",
        "        self.ffn = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "    \n",
        "model = Decoder(vocab_size, emb_dim, n_blocks, num_heads).to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1sqtR2hnllC",
        "outputId": "61a82ee2-38b7-4489-b509-ab5d367b9635"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/7000 [01:46<207:29:52, 106.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 10.4815, val loss 10.4651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 1001/7000 [17:18<54:21:50, 32.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 1000: train loss 4.7261, val loss 4.9121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▊       | 2001/7000 [32:49<45:18:58, 32.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2000: train loss 4.3087, val loss 4.5388\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 3001/7000 [48:19<36:10:16, 32.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 3000: train loss 4.0694, val loss 4.3424\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 4001/7000 [1:03:50<27:10:21, 32.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 4000: train loss 3.8968, val loss 4.2468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████▏  | 5001/7000 [1:19:21<18:06:55, 32.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 5000: train loss 3.7571, val loss 4.1734\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 6001/7000 [1:34:52<9:02:52, 32.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 6000: train loss 3.6210, val loss 4.1113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 6999/7000 [1:48:35<00:00,  1.22it/s]"
          ]
        }
      ],
      "source": [
        "for i in tqdm(range(max_iters)):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    \n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if i % eval_interval == 0 or i == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2uNfuyOqsfK",
        "outputId": "336e63a6-de31-40a6-f21b-320edc67789c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Then they both moved to  themselves intently.  They\n",
            "        6.  Something Right Somewhere\n",
            "   7.  Mostly, 18s, Get up, and th morose to finish\n",
            "    City, of dealers upon their mountain admirers, and  A wretched man dying\n",
            "         seem cruelablepast Home\n",
            "       Something Right Somewhere,\n",
            "          F. WODSNAPPER Place\n",
            "         CHAPTER VI\n",
            "                 CAN\n",
            "                                       EMMA MICAWBER.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"I thank you for advice at last, Mademoiselle Hortense, I do NOT understand him?\"\n",
            "\n",
            "\"It had just been what he had done of himself and he would have scented\n",
            "his poverty from me and to keep him at all the same hour (I ought for the\n",
            "first time, for my wanting to render the same Margate of the same as to my\n",
            "childish invention, but I build of paper to my head; and the beloved\n",
            "laceration in his hand, sparing by leaves, and grow what blood is to be with\n",
            "you! Come! You wouldnt if I got a tooth out of this poor creatures\n",
            "old year if I divul me away.\n",
            "\n",
            "Dont stir from; dont ye carry a pillow! kept a song asunder: with\n",
            "all her trials in a little while or for his good Eugene?\n",
            "\n",
            "His face was put into a fancy for the first time, was to make out\n",
            "between it, and give it up.  A smile crossed off his broor.\n",
            "\n",
            "Thith you thee a mystery and State your years, Rachael?\n",
            "\n",
            "I have thowt, and he said, and keep himself before me.  Go away, I rayther and\n",
            "goesemed likely to be a lawyer, pith die!  Therell be nobody\n",
            "toll be a woman, Ill tell you you must put it in a wrong way?\n",
            "     Now, if youll only know what to do here, and give it out for\n",
            "the job\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "text = \"\"\n",
        "context = torch.tensor(enc.encode(text), dtype=torch.long, device=device).view(1, -1)\n",
        "print(enc.decode(model.generate(context, n_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMc0e1ohLpAbqVDM0Vz5IRS",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
